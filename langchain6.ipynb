{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os \n",
    "openapi_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatAnthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "llm = OpenAI(openai_api_key=openapi_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={} template='Tell me a python Trick'\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables = [],template='Tell me a python Trick'\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={} template='Tell me a python Trick'\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Tell me a python Trick\")\n",
    "prompt.format()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response : \n",
      "\n",
      "Once upon a time, in a small village in Nepal, there lived a young girl named Maya. She was known for her kind heart and helpful nature. She lived with her parents and two younger siblings in a simple but happy home.\n",
      "\n",
      "One day, while Maya was out collecting firewood in the forest, she came across an old woman sitting under a tree. The woman seemed tired and weak, and Maya could see that she was hungry. Without hesitation, Maya offered the woman some food and water from her basket.\n",
      "\n",
      "The woman gratefully accepted and thanked Maya, saying, \"You are a kind and generous girl, Maya. As a reward for your kindness, I will grant you three wishes.\"\n",
      "\n",
      "Maya was surprised but also excited. She thought carefully about her wishes and decided to use them to help her family and the people in her village. For her first wish, she asked for her family to always have enough food and never go hungry. The old woman smiled and granted her wish.\n",
      "\n",
      "For her second wish, Maya asked for her village to have clean drinking water. The old woman nodded and granted her wish, saying, \"Your village will now have a clean and abundant water source that will never run dry.\"\n",
      "\n",
      "For her final wish, Maya asked for her village to have\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variable=['story'],template='Tell me a chath maiya story'\n",
    ")\n",
    "format_prompt = prompt.format()\n",
    "response = llm.invoke(format_prompt)\n",
    "print('Response :',response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gradio in c:\\users\\nanhi deo\\appdata\\roaming\\python\\python311\\site-packages (5.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\nanhi deo\\appdata\\roaming\\python\\python311\\site-packages (2.2.3)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "pip install gradio pandas sklearn PyMuPDF transformers together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\components\\chatbot.py:228: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\queueing.py\", line 622, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\blocks.py\", line 2014, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\blocks.py\", line 1565, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\utils.py\", line 813, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\chat_interface.py\", line 638, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2405, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 914, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Local\\Temp\\ipykernel_1192\\1266462931.py\", line 31, in generate_response\n",
      "    top_results = search_knowledge_base(query)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Local\\Temp\\ipykernel_1192\\1266462931.py\", line 23, in search_knowledge_base\n",
      "    query_vector = vectorizer.transform([query])\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2115, in transform\n",
      "    X = super().transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1417, in transform\n",
      "    _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1259, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "                   ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py\", line 108, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "          ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_extraction\\text.py\", line 66, in _preprocess\n",
      "    doc = doc.lower()\n",
      "          ^^^^^^^^^\n",
      "AttributeError: 'dict' object has no attribute 'lower'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "\n",
    "# Sample documents for demonstration\n",
    "documents = [\n",
    "    \"Artificial Intelligence is a branch of computer science that aims to create systems capable of performing tasks that require human intelligence.\",\n",
    "    \"Machine Learning is a subset of AI that focuses on the development of algorithms that allow computers to learn from data.\",\n",
    "    \"Natural Language Processing (NLP) involves the interaction between computers and humans using natural language.\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame from documents\n",
    "df = pd.DataFrame(documents, columns=[\"text\"])\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "def search_knowledge_base(query, top_k=3):\n",
    "    \"\"\"Search the knowledge base and return top_k relevant documents.\"\"\"\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    top_k_indices = similarity_scores.argsort()[-top_k:][::-1]  # Get top k indices\n",
    "    return df['text'].iloc[top_k_indices].tolist()\n",
    "\n",
    "def generate_response(query, history):\n",
    "    \"\"\"Generate a response for the user query.\"\"\"\n",
    "    # Search the knowledge base\n",
    "    top_results = search_knowledge_base(query)\n",
    "    \n",
    "    # Combine the user query with the top results (you can customize this)\n",
    "    combined_input = f\"Query: {query}\\n\\nResults:\\n\" + \"\\n\".join(top_results)\n",
    "    \n",
    "    # Call the multimodal model (replace with actual API call to Together AI)\n",
    "    # Here we simulate the response as we don't have an actual model integrated\n",
    "    # For demonstration purposes, you can replace this with actual inference logic.\n",
    "    response = f\"Combined Input:\\n{combined_input}\\n\\nSimulated Model Response: [Simulated response based on input]\"\n",
    "    \n",
    "    history.append((query, response))  # Update the chat history\n",
    "    return response\n",
    "\n",
    "# Gradio Interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=generate_response, \n",
    "    examples=[{\"text\": \"Tell me about Artificial Intelligence\", \"files\": []}, \n",
    "              {\"text\": \"What is Machine Learning?\", \"files\": []}], \n",
    "    title=\"Knowledge Base QA System\", \n",
    "    description=\"Ask questions about AI, Machine Learning, and NLP. The system will search the knowledge base and provide relevant answers.\",\n",
    "    multimodal=True\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Artificial Intelligence is a branch of computer science that aims to create systems capable of performing tasks that require human intelligence.\",\n",
    "    \"Machine Learning is a subset of AI that focuses on the development of algorithms that allow computers to learn from data.\",\n",
    "    \"Natural Language Processing (NLP) involves the interaction between computers and humans using natural language.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [doc for doc in documents if doc.strip()]  # Remove empty documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\components\\chatbot.py:228: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\queueing.py\", line 622, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\blocks.py\", line 2014, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\blocks.py\", line 1565, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\utils.py\", line 813, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\gradio\\chat_interface.py\", line 638, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2405, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Roaming\\Python\\Python311\\site-packages\\anyio\\_backends\\_asyncio.py\", line 914, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Local\\Temp\\ipykernel_1192\\582542178.py\", line 33, in generate_response\n",
      "    top_results = search_knowledge_base(query)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Nanhi Deo\\AppData\\Local\\Temp\\ipykernel_1192\\582542178.py\", line 23, in search_knowledge_base\n",
      "    if not query.strip():\n",
      "           ^^^^^^^^^^^\n",
      "AttributeError: 'dict' object has no attribute 'strip'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample documents for demonstration\n",
    "documents = [\n",
    "    \"Artificial Intelligence is a branch of computer science that aims to create systems capable of performing tasks that require human intelligence.\",\n",
    "    \"Machine Learning is a subset of AI that focuses on the development of algorithms that allow computers to learn from data.\",\n",
    "    \"Natural Language Processing (NLP) involves the interaction between computers and humans using natural language.\"\n",
    "]\n",
    "\n",
    "# Filter out empty documents\n",
    "documents = [doc for doc in documents if doc.strip()]\n",
    "df = pd.DataFrame(documents, columns=[\"text\"])\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "def search_knowledge_base(query, top_k=3):\n",
    "    \"\"\"Search the knowledge base and return top_k relevant documents.\"\"\"\n",
    "    if not query.strip():\n",
    "        return []  # Return empty if the query is empty\n",
    "\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    top_k_indices = similarity_scores.argsort()[-top_k:][::-1]  # Get top k indices\n",
    "    return df['text'].iloc[top_k_indices].tolist()\n",
    "\n",
    "def generate_response(query, history):\n",
    "    \"\"\"Generate a response for the user query.\"\"\"\n",
    "    top_results = search_knowledge_base(query)\n",
    "    \n",
    "    combined_input = f\"Query: {query}\\n\\nResults:\\n\" + \"\\n\".join(top_results) if top_results else \"No relevant results found.\"\n",
    "    \n",
    "    # Simulated model response (replace with actual API call)\n",
    "    response = f\"Combined Input:\\n{combined_input}\\n\\nSimulated Model Response: [Simulated response based on input]\"\n",
    "    \n",
    "    history.append((query, response))  # Update the chat history\n",
    "    return response\n",
    "\n",
    "# Gradio Interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=generate_response, \n",
    "    examples=[{\"text\": \"Tell me about Artificial Intelligence\", \"files\": []}, \n",
    "              {\"text\": \"What is Machine Learning?\", \"files\": []}], \n",
    "    title=\"Knowledge Base QA System\", \n",
    "    description=\"Ask questions about AI, Machine Learning, and NLP. The system will search the knowledge base and provide relevant answers.\",\n",
    "    multimodal=True\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
